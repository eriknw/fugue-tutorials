{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Fugue on Databricks\n",
    "\n",
    "A lot of Spark users use the `databricks-connect` library to execute Spark commands on a Databricks cluster instead of a local session. `databricks-connect` replaces the local installation of `pyspark` and makes `pyspark` code get executed on the cluster, allowing users to use the cluster directly from their local machine. \n",
    "\n",
    "In this tutorial, we will go through the following steps:\n",
    "\n",
    "1. Setting up a Databricks cluster\n",
    "2. Installing and configuring `databricks-connect`\n",
    "3. Using Fugue with a Spark backend\n",
    "\n",
    "## Setup Workspace\n",
    "\n",
    "Databricks is available across all three major cloud providers (AWS, Azure, GCP). In general, it will involve going to the Marketplace of your cloud vendor, and then signing up for Databricks.\n",
    "\n",
    "For more information, you can look at the following cloud-specific documentation.\n",
    "* https://databricks.com/product/aws\n",
    "* https://databricks.com/product/azure\n",
    "* https://databricks.com/product/google-cloud\n",
    "\n",
    "The picture below is what entering a workspace will look like\n",
    "\n",
    "![Databricks workspace](https://miro.medium.com/max/1400/1*YUF7X7cLLse1YDy2dTFh1Q.png)\n",
    "\n",
    "## Create a Cluster\n",
    "\n",
    "From here, you can create a cluster by clicking the “New Cluster” link. The cluster will serve as the backend for our Spark commands. With the databricks-connectpackage that we will use later, we can connect our laptop to the cluster automatically just by using Spark commands. The computation graph is built locally and then sent to the cluster for execution.\n",
    "\n",
    "![Creating a cluster](https://miro.medium.com/max/1400/1*CHFIHFOBugQYEQBBQugDZA.png)\n",
    "\n",
    "Note when creating a cluster that you can start out by reducing the Worker Type. Clusters can be very expensive! Be sure to lower the worker type to something reasonable for your workload. Also, you can enable autoscaling and terminating the cluster after a certain number of minutes of inactivity.\n",
    "\n",
    "\n",
    "## Installing Fugue\n",
    "\n",
    "In order to use Fugue on the cluster from our local environment, we need to install it on the cluster that we created. The easiest way to do this is to navigate the the libraries tab and add the package there. Below is an image of the tab.\n",
    "\n",
    "![Installing Fugue](https://miro.medium.com/max/1400/1*z1AO5S17BxWFE1YGwj8RLQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks-connect\n",
    "\n",
    "`databricks-connect` is a library that Databricks provides to run Spark commands on the cluster. The content here will just be a summary, but the full guide to installing databricks-connect can be found [here](https://docs.databricks.com/dev-tools/databricks-connect.html).\n",
    "\n",
    "`databricks-connect` can be installed using `pip`:\n",
    "\n",
    "```\n",
    "pip uninstall pyspark\n",
    "pip install databricks-connect\n",
    "```\n",
    "\n",
    "Pyspark needs to be uninstalled because it conflicts with `databricks-connect`. databricks-connect will replace your pyspark installation. This means that `import pyspark` will load the databricks-connect version of PySpark and all succeeding Spark commands are sent to the cluster for execution.\n",
    "\n",
    "Note that the version of `databricks-connect` must match the Databricks Runtime Version of your cluster. Otherwise, you will run into errors and the code will not be able to execute correctly.\n",
    "\n",
    "## Configuring the Cluster\n",
    "\n",
    "Now that you have a cluster created from the first step and `databricks-connect` installed, you can configure the cluster by doing `databricks-connect configure`. \n",
    "\n",
    "There are more details where to get the relevant pieces of information in their documentation here. For my example on AWS, it looked like this:\n",
    "\n",
    "![databricks-connect configure](https://miro.medium.com/max/1108/1*sMNZBrA2V64mYmdgiBFQbQ.png)\n",
    "\n",
    "You can verify if this worked by using `databricks-connect test`, and then you should see a message that all tests passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fugue and databricks-connect\n",
    "\n",
    "After setting the cluster up and configuring `databricks-connect` to point to the cluster, there is no added effort needed to connect Fugue to your Spark cluster. The `SparkExecutionEngine` imports `pyspark`, meaning that it will import the `databricks-connect` configuration under the hood and use the configured cluster. Fugue works with `databricks-connect` seamlessly, allowing for convenient switching between local development and a remote cluster.\n",
    "\n",
    "The code below will execute on the Databricks cluster if you followed the steps above. In order to run this locally, simple use the default `NativeExecutionEngine` instead of the `SparkExecutionEngine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|numbers| words|reversed|\n",
      "+-------+------+--------+\n",
      "|      1| hello|   olleh|\n",
      "|      2| world|   dlrow|\n",
      "|      3| apple|   elppa|\n",
      "|      4|banana|  ananab|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fugue import transform\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "data = pd.DataFrame({'numbers':[1,2,3,4], 'words':['hello','world','apple','banana']})\n",
    "\n",
    "# schema: *, reversed:str\n",
    "def reverse_word(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['reversed'] = df['words'].apply(lambda x: x[::-1])\n",
    "    return df\n",
    "\n",
    "spark_df = transform(data, reverse_word, engine=SparkExecutionEngine())\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Configuration\n",
    "\n",
    "Most `databricks-connect` users add additional Spark configurations on the cluster through the DAtabricks UI. If additional configuration is needed, it can be provided with the following syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "spark_session = (SparkSession\n",
    "                 .builder\n",
    "                 .config(\"spark.executor.cores\",4)\n",
    "                 .config(\"fugue.dummy\",\"dummy\")\n",
    "                 .getOrCreate())\n",
    "\n",
    "engine = SparkExecutionEngine(spark_session, {\"additional_conf\":\"abc\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Fugue-sql on the Cluster\n",
    "\n",
    "Because Fugue-sql also just uses the `SparkExecutionEngine`, it can also be easily executed on a remote cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "require([\"codemirror/lib/codemirror\"]);\n",
       "function set(str) {\n",
       "    var obj = {}, words = str.split(\" \");\n",
       "    for (var i = 0; i < words.length; ++i) obj[words[i]] = true;\n",
       "    return obj;\n",
       "  }\n",
       "var fugue_keywords = \"fill hash rand even presort persist broadcast params process output outtransform rowcount concurrency prepartition zip print title save append parquet csv json single checkpoint weak strong deterministic yield connect sample seed take sub callback dataframe file\";\n",
       "CodeMirror.defineMIME(\"text/x-fsql\", {\n",
       "    name: \"sql\",\n",
       "    keywords: set(fugue_keywords + \" add after all alter analyze and anti archive array as asc at between bucket buckets by cache cascade case cast change clear cluster clustered codegen collection column columns comment commit compact compactions compute concatenate cost create cross cube current current_date current_timestamp database databases data dbproperties defined delete delimited deny desc describe dfs directories distinct distribute drop else end escaped except exchange exists explain export extended external false fields fileformat first following for format formatted from full function functions global grant group grouping having if ignore import in index indexes inner inpath inputformat insert intersect interval into is items join keys last lateral lazy left like limit lines list load local location lock locks logical macro map minus msck natural no not null nulls of on optimize option options or order out outer outputformat over overwrite partition partitioned partitions percent preceding principals purge range recordreader recordwriter recover reduce refresh regexp rename repair replace reset restrict revoke right rlike role roles rollback rollup row rows schema schemas select semi separated serde serdeproperties set sets show skewed sort sorted start statistics stored stratify struct table tables tablesample tblproperties temp temporary terminated then to touch transaction transactions transform true truncate unarchive unbounded uncache union unlock unset use using values view when where window with\"),\n",
       "    builtin: set(\"date datetime tinyint smallint int bigint boolean float double string binary timestamp decimal array map struct uniontype delimited serde sequencefile textfile rcfile inputformat outputformat\"),\n",
       "    atoms: set(\"false true null\"),\n",
       "    operatorChars: /^[*\\/+\\-%<>!=~&|^]/,\n",
       "    dateSQL: set(\"time\"),\n",
       "    support: set(\"ODBCdotTable doubleQuote zerolessFloat\")\n",
       "  });\n",
       "\n",
       "CodeMirror.modeInfo.push( {\n",
       "            name: \"Fugue SQL\",\n",
       "            mime: \"text/x-fsql\",\n",
       "            mode: \"sql\"\n",
       "          } );\n",
       "\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "    codecell.CodeCell.options_default.highlight_modes['magic_text/x-fsql'] = {'reg':[/%%fsql/]} ;\n",
       "    Jupyter.notebook.events.on('kernel_ready.Kernel', function(){\n",
       "    Jupyter.notebook.get_cells().map(function(cell){\n",
       "        if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "    });\n",
       "  });\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fugue_notebook import setup\n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numbers</th>\n",
       "      <th>words</th>\n",
       "      <th>reversed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>hello</td>\n",
       "      <td>olleh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>world</td>\n",
       "      <td>dlrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>apple</td>\n",
       "      <td>elppa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>banana</td>\n",
       "      <td>ananab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   numbers   words reversed\n",
       "0        1   hello    olleh\n",
       "1        2   world    dlrow\n",
       "2        3   apple    elppa\n",
       "3        4  banana   ananab"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: numbers:long,words:str,reversed:str</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%fsql spark\n",
    "SELECT *\n",
    "  FROM data\n",
    "TRANSFORM USING reverse_word\n",
    " PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Here we have shown how to connect Fugue to a Databricks cluster. Using Fugue along with `databricks-connect` provides the best of both worlds, only utilizing the cluster when needed. \n",
    "\n",
    "`databricks-connect` can slow down developer productitity and increase compute costs because all the Spark code becomes configured to run on the cluster. Using Fugue, we can toggle between Fugue's default `NativeExecutionEngine` and `SparkExecutionEngine`. The default `NativeExecutionEngine` will run on local without using Spark and Fugue's `SparkExecutionEngine` will seamlessly use whatever `pyspark` is configured for the user.\n",
    "\n",
    "Fugue also allows for additional configuration of the underlying frameworks. We showed the syntax for passing a `SparkSession` to the `SparkExecutionEngine`."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
