{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e680c15",
   "metadata": {},
   "source": [
    "# Databricks\n",
    "\n",
    "Fugue works perfectly well with Databricks. This document assumes you already have Databricks service setup and you know the basic operations on Databricks. If that is not the case, please read [this blog](https://medium.com/plumbersofdatascience/using-fuguesql-on-spark-dataframes-with-databricks-ab4ffc419c3e) first to onboard Databricks.\n",
    "\n",
    "## Using Fugue on Databricks Notebooks\n",
    "\n",
    "Please follow [this blog](https://medium.com/plumbersofdatascience/using-fuguesql-on-spark-dataframes-with-databricks-ab4ffc419c3e). The only thing you need to pay attention to is that you must install `fugue` in Libraries. And you may add extras to the installation, but make sure you DO NOT include `spark`. For example, you should NOT install `fugue[spark]` because Databricks already installed its own PySpark.\n",
    "\n",
    "When you are on the notebook, the first step is to setup the environment (if you want to use fugue SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ed0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_notebook import setup\n",
    "setup(is_lab=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c126cac8",
   "metadata": {},
   "source": [
    "Then you can directly use `spark` as your execution engine because it is the magic variable in Databricks representing the `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244b098",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T07:48:02.495427Z",
     "start_time": "2022-06-13T07:48:02.489549Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# schema: *\n",
    "def dummy(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    return df\n",
    "\n",
    "tdf = pd.DataFrame(dict(a=[1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ce2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import transform\n",
    "\n",
    "spark_df = transform(tdf, dummy, engine=spark)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf55509",
   "metadata": {},
   "source": [
    "You can also use `fsql` magic. To use spark, just do `%%fsql spark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql spark\n",
    "CREATE [[0],[1]] SCHEMA a:int\n",
    "TRANSFORM USING dummy\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0162c40b",
   "metadata": {},
   "source": [
    "## Using Fugue On A Remote Notebook\n",
    "\n",
    "One important value of Fugue is to let you iterate your ideas on a local box that is not always connected with Databricks. It can save time and cost.\n",
    "\n",
    "Fugue can let you use Databricks as ephemeral Spark clusters, only when you need it, you can directly activate an existing cluster or create a new cluster from your notebook. Fugue can also automatically stop the remote cluster if it is created on the fly.\n",
    "\n",
    "### Installation\n",
    "\n",
    "In order to use Fugue in this mode. You must install the Databricks plugins.\n",
    "\n",
    "```\n",
    "pip uninstall pyspark\n",
    "pip install fugue-cloudprovider[databricks]\n",
    "```\n",
    "\n",
    "If you also need to access AWS or GCP, you should install the correspondent extras: `aws` or `gcp`. For example:\n",
    "\n",
    "```\n",
    "pip uninstall pyspark\n",
    "pip install fugue-cloudprovider[databricks,aws]\n",
    "```\n",
    "\n",
    "This will install both [databricks-connect](https://pypi.org/project/databricks-connect/) and [databricks-cli](https://pypi.org/project/databricks-cli/). We need databricks-connect to interact with the remote SparkSession and databricks-cli to manage the clusters.\n",
    "\n",
    "### Configuration\n",
    "\n",
    "Unfortunately, Databricks doesn't unify the requirements of the two packages. They need different ways to initialize. Fugue unifies them to some degree. We always require users to provide `host`, `token` and `cluster_id` because these are the common requirements for the two packages.\n",
    "\n",
    "For details how to get those values, please read this [Databricks document section](https://docs.databricks.com/dev-tools/databricks-connect.html#step-2-configure-connection-properties).\n",
    "\n",
    "You can provide them in two ways: using environment variables or config keys in python. \n",
    "\n",
    "| Parameter |  Config Key | Environment Variable | Default |\n",
    "| --- | --- | --- | --- |\n",
    "| Databricks Host | host | DATABRICKS_ADDRESS | *REQUIRED* |\n",
    "| Databricks Token | token | DATABRICKS_API_TOKEN | *REQUIRED* |\n",
    "| Org ID | org_id | DATABRICKS_ORG_ID | *empty* |\n",
    "| Port | port | DATABRICKS_ORG_ID | 15001 |\n",
    "\n",
    "Notice that the command\n",
    "\n",
    "```bash\n",
    "databricks-connect configure\n",
    "```\n",
    "\n",
    "will have no effect, the configured values through the command line will not be used. You don't have to run this command. The value of the config keys will have higher priority. For example, if you define a python dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5d5fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T08:23:15.164218Z",
     "start_time": "2022-06-13T08:23:15.161023Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"cluster_id\": \"abc\",\n",
    "    \"token\": \"mytoken\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27314e25",
   "metadata": {},
   "source": [
    "And if you have these environment variables:\n",
    "\n",
    "```\n",
    "DATABRICKS_API_TOKEN=\"def\"\n",
    "DATABRICKS_ADDRESS=\"https://dummy\"\n",
    "```\n",
    "\n",
    "Then Fugue will us the following for both databricks-connect and databricks-cli:\n",
    "\n",
    "```\n",
    "{'cluster_id': 'abc', 'token': 'mytoken', 'host': 'https://dummy'}\n",
    "```\n",
    "\n",
    "### Connecting to an existing cluster\n",
    "\n",
    "The above example is the minimal requirement to connect to an existing cluster. **An existing cluster on Databricks does not need to be running, Fugue will start the cluster if it is stopped**.\n",
    "\n",
    "The easiest way to get a remote session from a cluster id is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4fb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_databricks import init_db_spark\n",
    "\n",
    "conf = {\n",
    "    \"host\": \"https://dbc-38aaa459-faaf.cloud.databricks.com\",\n",
    "    \"token\": \"dapiecaaae64a727498daaaaafe1bace968a\",\n",
    "    \"cluster_id\": \"0612-191111-6fopaaaa\"\n",
    "}\n",
    "\n",
    "spark_session = init_db_spark(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30316ee2",
   "metadata": {},
   "source": [
    "You must make sure that `fugue` is in installed in `Libraries` of the cluster.\n",
    "\n",
    "`spark_session` is just a normal SparkSession, you can use it as the execution engine as we showed before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import transform\n",
    "\n",
    "spark_df = transform(tdf, dummy, engine=spark_session)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a0795c",
   "metadata": {},
   "source": [
    "Notice when using Fugue SQL cell, you should still use `%%fsql spark` meaning that getting the current spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace26a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql spark\n",
    "CREATE [[0],[1]] SCHEMA a:int\n",
    "TRANSFORM USING dummy\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa465684",
   "metadata": {},
   "source": [
    "But using it programmatically, you should use `spark_session`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_sql import fsql\n",
    "\n",
    "fsql(\"\"\"\n",
    "CREATE [[0],[1]] SCHEMA a:int\n",
    "TRANSFORM USING dummy\n",
    "PRINT\n",
    "\"\"\").run(spark_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b6eca7",
   "metadata": {},
   "source": [
    "There is an alternative way without using `init_db_spark`. You can use `db` as the engine, and `conf` as the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f8762",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T08:40:36.649174Z",
     "start_time": "2022-06-13T08:40:36.166799Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_df = transform(tdf, dummy, engine=\"db\", engine_conf=conf)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0798b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql db conf\n",
    "CREATE [[0],[1]] SCHEMA a:int\n",
    "TRANSFORM USING dummy\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsql(\"\"\"\n",
    "CREATE [[0],[1]] SCHEMA a:int\n",
    "TRANSFORM USING dummy\n",
    "PRINT\n",
    "\"\"\").run(\"db\", conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7067bedf",
   "metadata": {},
   "source": [
    "### Connecting to an ephemeral cluster\n",
    "\n",
    "With Fugue, users can provide the cluster spec json, Fugue can create an ephemeral cluster, run the job and stop the cluster. In this way, you will have zero interaction with Databricks UI.\n",
    "\n",
    "First let's see a config example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee091d6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T08:53:11.295146Z",
     "start_time": "2022-06-13T08:53:11.290415Z"
    }
   },
   "outputs": [],
   "source": [
    "spec = {\n",
    "    \"num_workers\": 1,\n",
    "    \"cluster_name\": \"test\", # Any name is fine, duplicated name is fine\n",
    "    \"spark_version\": \"10.4.x-scala2.12\", # Must match databricks-connect version\n",
    "    \"spark_conf\": {\n",
    "        \"spark.speculation\": \"true\"\n",
    "    },\n",
    "    \"aws_attributes\": {\n",
    "        \"first_on_demand\": 1,\n",
    "        \"availability\": \"SPOT\",\n",
    "        \"zone_id\": \"us-west-2c\",\n",
    "        \"spot_bid_price_percent\": 100,\n",
    "        \"ebs_volume_count\": 0\n",
    "    },\n",
    "    \"node_type_id\": \"i3.xlarge\",\n",
    "    \"driver_node_type_id\": \"i3.xlarge\",\n",
    "    \"ssh_public_keys\": [],\n",
    "    \"custom_tags\": {},\n",
    "    \"spark_env_vars\": { # Get AWS credentials from DB's secrect store\n",
    "        \"AWS_SECRET_ACCESS_KEY\": \"{{secrets/secret}}\",\n",
    "        \"AWS_ACCESS_KEY_ID\": \"{{secrets/access}}\"\n",
    "    },\n",
    "    \"autotermination_minutes\": 20, # This is another protection\n",
    "    \"enable_elastic_disk\": False,\n",
    "    \"runtime_engine\": \"STANDARD\"\n",
    "}\n",
    "\n",
    "conf = {\n",
    "    \"host\": \"https://dbc-38aaa459-faaf.cloud.databricks.com\",\n",
    "    \"token\": \"dapiecaaae64a727498daaaaafe1bace968a\",\n",
    "    \"cluster\": spec,\n",
    "    \"libraries\": [\n",
    "        {\n",
    "          \"pypi\": {\n",
    "            \"package\": \"fugue\" # must install fugue!!\n",
    "          }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5558d60",
   "metadata": {},
   "source": [
    "For details of cluster spec, please read [this document](https://docs.databricks.com/dev-tools/api/latest/clusters.html#create).\n",
    "\n",
    "For details of libraries spec, please read [this document](https://docs.databricks.com/dev-tools/api/latest/libraries.html#install).\n",
    "\n",
    "A tip to get the spec is to output JSON of one of your existing clusters created from UI and modify it for your need.\n",
    "\n",
    "You can use `init_db_spark` to initialize the Spark cluster, but it's not recommended because this approach does not stop the cluster for you. Instead, you can use `db_spark` with `with` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ccfebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_databricks import db_spark\n",
    "\n",
    "with db_spark(conf) as spark_session:\n",
    "    spark_df = transform(tdf, dummy, engine=spark_session)\n",
    "    spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506523a",
   "metadata": {},
   "source": [
    "The following approach will also guarantee the ephemeral cluster to be closed after use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f0dd5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T08:57:03.324789Z",
     "start_time": "2022-06-13T08:57:03.310423Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_df = transform(tdf, dummy, engine=\"db\", engine_conf=conf)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d3129",
   "metadata": {},
   "source": [
    "Fugue is able to identify if you are requesting an ephemeral cluster from the config. If `cluster` is provided, then it is ephemeral, otherwise, `cluster_id` must be provided, they can't both exist.\n",
    "\n",
    "A quick summary, we have 3 ways to connect to Databricks from a remote notebook:\n",
    "\n",
    "1. `init_db_spark` is better for existing clusters. It doesn't stop any cluster. Using this to get a `SparkSession` then it can be very intuitive.\n",
    "2. `engine=\"db\", engine_conf=conf` is great for both existing and ephemeral clusters. It will automatically stop the ephemeral cluster for you. It will auto start an existing cluster but will not stop it automatically.\n",
    "3. `with db_spark(conf) as spark:` is great for both existing and ephemeral clusters. It will automatically stop the ephemeral cluster for you. It will auto start an existing cluster but will not stop it automatically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
